# 模型设计

## 全连接层

对于全连接层的维度变换，一般采用2的次幂这种的变换形式。

对于属性比较多的情况一般采用先升维，然后降维的方式进行处理

# 模型保存

在 PyTorch 中，保存模型参数（权重和偏置等）通常有两种方式：

1. **保存整个模型**：包括模型的架构和模型的参数。
2. **仅保存模型的状态字典**（state_dict）：即模型参数。

### 1. 保存整个模型

这种方法简单直接，适合于你需要在后续的代码中重新创建相同的模型架构的情况。保存整个模型的代码如下：

```python
import torch

# 假设 model 是你已经训练好的模型实例
model = YourModelArchitecture()

# 保存整个模型
torch.save(model, 'model.pth')
```

### 2. 仅保存模型的状态字典

这种方式更灵活，因为它只保存模型的参数，而不保存模型架构本身。这意味着在加载时，你需要提供相同的模型架构定义。保存模型状态字典的代码如下：

```python
import torch

# 假设 model 是你已经训练好的模型实例
model = YourModelArchitecture()

# 保存模型的状态字典
torch.save(model.state_dict(), 'model_state_dict.pth')
```

### 加载模型

加载模型时，根据你保存的方式不同，也有两种不同的加载方式：

#### 加载整个模型

```python
# 加载整个模型
model = torch.load('model.pth')

# 如果是在没有GPU的环境下运行，并且模型是在GPU上保存的，则需要指定 map_location
model = torch.load('model.pth', map_location=torch.device('cpu'))
```

#### 加载模型的状态字典

```python
# 假设 model 是你已经定义好的模型架构实例
model = YourModelArchitecture()

# 加载模型的状态字典
model.load_state_dict(torch.load('model_state_dict.pth'))

# 如果是在没有GPU的环境下运行，并且模型是在GPU上保存的，则需要指定 map_location
model.load_state_dict(torch.load('model_state_dict.pth', map_location=torch.device('cpu')))
```

### 注意事项

- 在保存和加载模型时，确保使用的设备（CPU 或 GPU）与训练时使用的设备一致，否则可能需要指定 `map_location` 参数来确保模型可以在正确的设备上加载。
- 当模型包含自定义层或复杂结构时，保存整个模型可能更加方便。
- 保存状态字典更加通用，尤其是在部署到生产环境或者分享模型给其他人时。

以上就是保存和加载 PyTorch 模型的基本方法。选择哪种方法取决于你的具体需求。

# 如何处理多种数据类型（比如字符串和数值混搭）的数据集

使用Pandas！！！Numpy里的数据类型都是一致的。

使用教程：https://pandas.pydata.org/docs/user_guide/10min.html

# Pandas处理手册

## Pandas查看数据集里的空值情况

```python
import pandas as pd

# 读取CSV文件
df = pd.read_csv('your_file.csv')

# 查看哪些列含有至少一个NaN值
missing_values_columns = df.columns[df.isna().any()].tolist()
print("含有NaN值的列：", missing_values_columns)

# 计算每列的空值数量
missing_values_count = df.isna().sum()
print("每列的空值数量：\n", missing_values_count)
```

## 如何将dataframe中的某几列的值转化为numpy

### 列数很少的情况

如果你想将Pandas DataFrame中的某几列转换为NumPy数组，可以按照以下步骤进行操作：

#### 示例代码

首先，我们创建一个简单的DataFrame：

```python
import pandas as pd
import numpy as np

# 创建一个简单的DataFrame
data = {'A': [1, 2, 3],
        'B': [4, 5, 6],
        'C': [7, 8, 9],
        'D': ['x', 'y', 'z']}
df = pd.DataFrame(data)
print(df)
```

输出将会是：

```
   A  B  C  D
0  1  4  7  x
1  2  5  8  y
2  3  6  9  z
```

假设我们想将列 `'A'`, `'B'` 和 `'C'` 转换为NumPy数组。

#### 方法 1：使用 `.loc` 或 `.iloc`

你可以使用`.loc`来按列名选取数据，或者使用`.iloc`来按位置选取数据。这里我们使用`.loc`：

```python
# 选择特定的列
columns_to_convert = ['A', 'B', 'C']
selected_columns = df.loc[:, columns_to_convert]

# 转换为NumPy数组
np_array = selected_columns.values
print(np_array)
```

输出将是：

```
[[1 4 7]
 [2 5 8]
 [3 6 9]]
```

#### 方法 2：使用列表推导式

另一种方式是使用列表推导式来选择特定的列：

```python
# 选择特定的列并转换为NumPy数组
np_array = np.array([df[col] for col in ['A', 'B', 'C']])
print(np_array)
```

输出同样是：

```
[[1 4 7]
 [2 5 8]
 [3 6 9]]
```

#### 方法 3：使用 `.to_numpy()`

如果你希望使用 `.to_numpy()` 方法，并且可以指定数据类型：

```python
# 使用 .to_numpy() 方法并指定数据类型
np_array = df[['A', 'B', 'C']].to_numpy(dtype=np.float64)
print(np_array)
```

输出同样为：

```
[[1. 4. 7.]
 [2. 5. 8.]
 [3. 6. 9.]]
```

#### 注意事项

1. **数据类型**：如果你的DataFrame中包含非数值类型的列，那么转换后的数组将会是一个对象数组（object array）。如果需要特定类型的数组，确保只选择数值类型的列。
2. **内存管理**：转换为NumPy数组会占用额外的内存，特别是在处理大型数据集时需要注意。
3. **性能**：使用 `.loc` 或 `.iloc` 直接选择列并转换为 `.values` 或 `.to_numpy()` 是一种高效的方法，尤其是在选择连续的列时。

通过上述方法，你可以方便地将DataFrame中的特定列转换为NumPy数组。选择最适合你需求的方法即可。

### 如果列数很多，不太方便写成一个list，该怎么处理？

如果DataFrame中的列数很多，并且你不想手动列出所有列的名字，可以采用以下几种方法来选择特定的列并将其转换为NumPy数组：

#### 方法 1：使用条件选择列

如果你知道哪些列需要转换，可以根据某些条件来选择这些列。例如，假设你需要选择所有以特定字符开头的列，或者除了某些列之外的所有列。

#### 选择以特定字符开头的列

```python
# 假设需要选择所有以 'A' 开头的列
columns_to_convert = [col for col in df.columns if col.startswith('A')]
np_array = df.loc[:, columns_to_convert].values
print(np_array)
```

#### 选择除特定列外的所有列

```python
# 假设需要排除 'D' 列
columns_to_exclude = ['D']
columns_to_convert = [col for col in df.columns if col not in columns_to_exclude]
np_array = df.loc[:, columns_to_convert].values
print(np_array)
```

#### 方法 2：使用列索引

如果你知道需要转换的列的位置索引，可以直接使用这些索引来选择列。

```python
# 假设需要选择第0、1、2列
column_indices = [0, 1, 2]
columns_to_convert = df.columns[column_indices]
np_array = df.loc[:, columns_to_convert].values
print(np_array)
```

#### 方法 3：使用 `.to_numpy()` 方法

如果你使用 `.to_numpy()` 方法，可以选择特定的列：

```python
# 使用 .to_numpy() 方法
column_indices = [0, 1, 2]  # 假设需要选择第0、1、2列
columns_to_convert = df.columns[column_indices]
np_array = df.loc[:, columns_to_convert].to_numpy()
print(np_array)
```

#### 方法 4：使用DataFrame的 `.items()` 方法

如果你需要根据某些逻辑条件选择列，可以使用 `.items()` 方法来遍历DataFrame中的列，并根据条件选择需要的列。

```python
# 根据某些逻辑条件选择列
columns_to_convert = []
for col_name, col_data in df.items():
    # 假设选择所有整数类型的列
    if col_data.dtype == np.int64:
        columns_to_convert.append(col_name)

np_array = df.loc[:, columns_to_convert].values
print(np_array)
```

#### 方法 5：使用 `.filter()` 方法

`filter()` 方法可以让你根据某些规则来选择列，例如选择以特定字符串开头的列：

```python
# 选择所有以 'A' 开头的列
columns_to_convert = df.filter(regex='^A').columns
np_array = df.loc[:, columns_to_convert].values
print(np_array)
```

#### 示例代码汇总

下面是一个综合示例，展示了如何选择特定的列并将它们转换为NumPy数组：

```python
import pandas as pd
import numpy as np

# 创建一个简单的DataFrame
data = {'A1': [1, 2, 3],
        'A2': [4, 5, 6],
        'B': [7, 8, 9],
        'C': [10, 11, 12],
        'D': ['x', 'y', 'z']}
df = pd.DataFrame(data)

# 选择所有以 'A' 开头的列
columns_to_convert = [col for col in df.columns if col.startswith('A')]
np_array = df.loc[:, columns_to_convert].values
print(np_array)

# 选择除 'D' 列外的所有列
columns_to_exclude = ['D']
columns_to_convert = [col for col in df.columns if col not in columns_to_exclude]
np_array = df.loc[:, columns_to_convert].values
print(np_array)

# 选择第0、1、2列
column_indices = [0, 1, 2]
columns_to_convert = df.columns[column_indices]
np_array = df.loc[:, columns_to_convert].values
print(np_array)

# 选择所有整数类型的列
columns_to_convert = []
for col_name, col_data in df.items():
    if col_data.dtype == np.int64:
        columns_to_convert.append(col_name)
np_array = df.loc[:, columns_to_convert].values
print(np_array)

# 使用 .filter() 方法选择所有以 'A' 开头的列
columns_to_convert = df.filter(regex='^A').columns
np_array = df.loc[:, columns_to_convert].values
print(np_array)
```

通过上述方法，你可以根据具体情况选择最适合的方式来选择DataFrame中的特定列，并将它们转换为NumPy数组。这些方法可以帮助你处理大量列的情况，而无需手动列出所有列的名字。

## 一个dataframe中某一列全是字符串，我想按照如下的字典方式，将其中的值更换为字符串对应的数值

要将DataFrame中某一列的字符串值替换为字典中对应的数值，可以使用Pandas提供的多种方法。以下是几种常用的替换方法：

### 方法 1：使用 `.map()` 方法

`.map()` 方法是一个非常直观的方法，可以直接将字典映射到DataFrame的某一列上：

```python
import pandas as pd

# 创建一个示例DataFrame
data = {
    'class_column': ['Class_1', 'Class_2', 'Class_3', 'Class_1', 'Class_4'],
    'other_column': [10, 20, 30, 40, 50]
}
df = pd.DataFrame(data)

# 定义替换字典
class_target = {
    "Class_1": 1,
    "Class_2": 2,
    "Class_3": 3,
    "Class_4": 4,
    "Class_5": 5,
    "Class_6": 6,
    "Class_7": 7,
    "Class_8": 8,
    "Class_9": 9
}

# 使用.map()方法替换列中的值
df['class_column'] = df['class_column'].map(class_target)

print(df)
```

### 方法 2：使用 `.replace()` 方法

`.replace()` 方法也是一个常用的方法，可以替换DataFrame中的值：

```python
# 使用.replace()方法替换列中的值
df['class_column'] = df['class_column'].replace(class_target)

print(df)
```

### 方法 3：使用 `pd.Series.map()` 方法

你也可以使用 `pd.Series.map()` 方法来替换值：

```python
# 将字典转换为Series，然后使用.map()方法
mapping_series = pd.Series(class_target)
df['class_column'] = df['class_column'].map(mapping_series)

print(df)
```

### 方法 4：使用 `pd.Categorical.map()` 方法

如果你的列是类别型（categorical）的，可以使用 `pd.Categorical.map()` 方法：

```python
# 将列转换为类别型，并使用.map()方法
df['class_column'] = pd.Categorical(df['class_column']).codes + 1
print(df)
```

### 方法 5：使用 `apply()` 方法

如果你需要更复杂的替换逻辑，可以使用 `apply()` 方法：

```python
# 使用.apply()方法
def replace_class(val):
    return class_target.get(val, None)

df['class_column'] = df['class_column'].apply(replace_class)

print(df)
```

### 完整示例代码

以下是完整示例代码，演示了如何将DataFrame中某一列的字符串值替换为字典中对应的数值：

```python
import pandas as pd

# 创建一个示例DataFrame
data = {
    'class_column': ['Class_1', 'Class_2', 'Class_3', 'Class_1', 'Class_4'],
    'other_column': [10, 20, 30, 40, 50]
}
df = pd.DataFrame(data)

# 定义替换字典
class_target = {
    "Class_1": 1,
    "Class_2": 2,
    "Class_3": 3,
    "Class_4": 4,
    "Class_5": 5,
    "Class_6": 6,
    "Class_7": 7,
    "Class_8": 8,
    "Class_9": 9
}

# 使用.map()方法替换列中的值
df['class_column'] = df['class_column'].map(class_target)

print(df)
```

输出结果：

```
  class_column  other_column
0           1             10
1           2             20
2           3             30
3           1             40
4           4             50
```

通过上述方法之一，你可以方便地将DataFrame中某一列的字符串值替换为对应的数值。选择最适合你需求的方法即可。

## tensor数据通过dataframe写入csv当中



# 如何处理数据集中某些样本的某些特征为空值？

## 直接删

如果你的数据集中空值不多，且删除这些行或列不会对整体数据集造成太大影响，可以考虑删除这些行或列。（**首先进行尝试**）

```python
import pandas as pd

# 读取 CSV 文件
df = pd.read_csv('data.csv')

# 删除含有空值的行
df = df.dropna()

# 或者删除含有空值的列
df = df.dropna(axis=1)
```

## 填充空值

填充空值：如果删除空值会导致数据丢失过多，你可以选择用某种值填充空值。常见的填充方法包括：

* **用均值、中位数或众数填充**：适用于数值特征。
* **用特定值填充**：例如，用 0 或 -1 填充。
* **前向填充或后向填充**：用前一个或后一个非空值填充。

**使用Numpy：**

首先，你需要使用 `numpy` 的 `genfromtxt` 或 `loadtxt` 函数来读取 CSV 文件。由于 `numpy` 本身不直接支持处理字符串类型的缺失值，你可以先将缺失值表示为某种特定的标记（如 `"NaN"`），然后再进行处理。

### 均值

### 1. 读取 CSV 文件
首先，你需要使用 `numpy` 的 `genfromtxt` 或 `loadtxt` 函数来读取 CSV 文件。由于 `numpy` 本身不直接支持处理字符串类型的缺失值，你可以先将缺失值表示为某种特定的标记（如 `"NaN"`），然后再进行处理。

```python
import numpy as np

# 读取 CSV 文件，将缺失值表示为 "NaN"
data = np.genfromtxt('data.csv', delimiter=',', dtype=None, encoding='utf-8', missing_values='', filling_values=np.nan)
```

### 2. 检查并处理空值
接下来，你可以使用 `numpy` 的功能来检查和处理这些空值。

#### 2.1 删除含有空值的行或列
如果你希望删除含有空值的行或列，可以使用布尔索引。

```python
# 删除含有空值的行
data = data[~np.isnan(data).any(axis=1)]

# 删除含有空值的列
data = data[:, ~np.isnan(data).any(axis=0)]
```

#### 2.2 用均值、中位数或众数填充
对于数值特征，你可以计算均值、中位数或众数，并用这些值来填充空值。

```python
# 用均值填充数值特征
numeric_columns = [1, 2]  # 假设第1列和第2列是数值特征
for col in numeric_columns:
    mean_value = np.nanmean(data[:, col])
    data[:, col][np.isnan(data[:, col])] = mean_value

# 用中位数填充数值特征
for col in numeric_columns:
    median_value = np.nanmedian(data[:, col])
    data[:, col][np.isnan(data[:, col])] = median_value

# 用众数填充分类特征
categorical_columns = [0]  # 假设第0列是分类特征
for col in categorical_columns:
    mode_value, _ = np.unique(data[:, col][~np.isnan(data[:, col])], return_counts=True)
    mode_value = mode_value[np.argmax(_)]
    data[:, col][np.isnan(data[:, col])] = mode_value
```

`mean_value = np.nanmean(data[:, col])`:

- `data[:, col]`：这表示从 `data` 数组中选择第 `col` 列的所有行。`data` 是一个二维数组，`data[:, col]` 会返回一个一维数组，包含第 `col` 列的所有元素。
- `np.nanmean(...)`：这是 `numpy` 中的一个函数，用于计算数组的均值，但会忽略 NaN 值。`np.nanmean` 的参数是一个数组，它会计算这个数组中所有非 NaN 值的均值。

因此，`mean_value = np.nanmean(data[:, col])` 这行代码的作用是计算第 `col` 列中所有非 NaN 值的均值，并将结果存储在变量 `mean_value` 中。

`data[:, col][np.isnan(data[:, col])] = mean_value`

- `data[:, col]`：同样，这表示从 `data` 数组中选择第 `col` 列的所有行。
- `np.isnan(data[:, col])`：这是一个布尔掩码，它会检查 `data[:, col]` 中的每个元素是否为 NaN。返回的结果是一个布尔数组，其中每个元素对应 `data[:, col]` 中相应位置的元素是否为 NaN。
- `data[:, col][np.isnan(data[:, col])]`：这是一个布尔索引操作，它会选择 `data[:, col]` 中所有为 NaN 的元素。
- `= mean_value`：这行代码将这些选中的 NaN 元素替换为 `mean_value`。

因此，`data[:, col][np.isnan(data[:, col])] = mean_value` 这行代码的作用是将第 `col` 列中所有的 NaN 值替换为该列的均值 `mean_value`。

#### 2.3 用特定值填充

你可以用特定的值来填充空值，例如 0 或 -1。

```python
# 用 0 填充数值特征
for col in numeric_columns:
    data[:, col][np.isnan(data[:, col])] = 0

# 用 "Unknown" 填充分类特征
for col in categorical_columns:
    data[:, col][np.isnan(data[:, col])] = "Unknown"
```

#### 2.4 前向填充或后向填充
前向填充或后向填充可以通过 `numpy` 的 `nan_to_num` 和一些自定义逻辑来实现。

```python
# 前向填充
def forward_fill(arr):
    mask = np.isnan(arr)
    idx = np.where(~mask, np.arange(mask.shape[0]), 0)
    np.maximum.accumulate(idx, out=idx)
    return arr[idx]

# 后向填充
def backward_fill(arr):
    return np.flip(forward_fill(np.flip(arr)))

# 对每一列进行前向填充
for col in range(data.shape[1]):
    if np.issubdtype(data[:, col].dtype, np.number):
        data[:, col] = forward_fill(data[:, col])

# 对每一列进行后向填充
for col in range(data.shape[1]):
    if np.issubdtype(data[:, col].dtype, np.number):
        data[:, col] = backward_fill(data[:, col])
```

### 3. 转换回原始数据类型
在处理完空值后，你可能需要将数据转换回原始的数据类型。

```python
# 将数据转换回原始数据类型
data = data.astype(dtype=None)  # 你可以根据需要指定具体的 dtype
```

### 4. 保存处理后的数据
最后，你可以将处理后的数据保存到一个新的 CSV 文件中。

```python
# 保存处理后的数据
np.savetxt('processed_data.csv', data, delimiter=',', fmt='%s')
```

### 总结
虽然 `numpy` 在处理缺失值方面不如 `pandas` 那么直观和强大，但它仍然提供了足够的工具来完成这项任务。通过上述方法，你可以有效地处理 CSV 文件中的空值。选择哪种方法取决于你的具体需求和数据特性。

# 数据集中的一些信息带有分隔符，造成数据集读取的时候出现问题

在excel打开一个数据集的文件：

![image-20241015000818293](./assets/image-20241015000818293.png)

这是kaggle上的Titanic数据集，我尝试使用

```python
xy = np.genfromtxt(filepath, delimiter=',', dtype=None, encoding='utf-8', missing_values='', filling_values=np.nan)
```

读取的时候，发现了这样的错误：

![image-20241015000904114](./assets/image-20241015000904114.png)

说第二行读取的列数不是12，但是你看excel里面明明就是12列，然后我再尝试使用记事本打开：

![image-20241015001137906](./assets/image-20241015001137906.png)

在这个函数里我们使用的分隔符是','，在这里很有可能将名字里的，当做分隔符处理了。

# 模型训练过程中出现了空值！

<img src="./assets/image-20241015164332167.png" alt="image-20241015164332167" style="zoom:50%;" />

这个问题就出在了没有对模型有一个清晰的观察，肉眼观察excel表格呈现的csv文件导致。有些样本的存在一些空值，但是很少，样本数量一多，肉眼就不容易观察出来，因此应该借助计算看一下哪些属性出现了空值

在 `numpy` 中，你可以使用 `np.isnan` 函数来检测数组中的每个元素是否为 `NaN`，然后通过布尔索引或 `np.any` 来确定哪些列包含 `NaN` 值。以下是一些常见的方法：

### 方法 1: 使用 `np.isnan` 和 `np.any`
你可以使用 `np.isnan` 生成一个布尔数组，然后使用 `np.any` 沿着列（`axis=0`）检查每列是否存在 `NaN` 值。

```python
import numpy as np

# 示例数据
data = np.array([
    [1, 2, np.nan],
    [4, np.nan, 6],
    [7, 8, 9]
])

# 检查每一列是否存在 NaN 值
nan_columns = np.any(np.isnan(data), axis=0)

print(f"哪些列存在 NaN 值: {nan_columns}")
```

### 方法 2: 获取包含 `NaN` 值的列索引
如果你希望获取包含 `NaN` 值的列的索引，可以使用 `np.where` 函数。

```python
import numpy as np

# 示例数据
data = np.array([
    [1, 2, np.nan],
    [4, np.nan, 6],
    [7, 8, 9]
])

# 检查每一列是否存在 NaN 值
nan_columns = np.any(np.isnan(data), axis=0)

# 获取包含 NaN 值的列索引
nan_column_indices = np.where(nan_columns)[0]

print(f"包含 NaN 值的列索引: {nan_column_indices}")
```

### 方法 3: 获取包含 `NaN` 值的列名（如果已知列名）
如果你有一个列名列表，并且希望获取包含 `NaN` 值的列名，可以结合上述方法和列名列表。

```python
import numpy as np

# 示例数据
data = np.array([
    [1, 2, np.nan],
    [4, np.nan, 6],
    [7, 8, 9]
])

# 列名列表
column_names = ['col1', 'col2', 'col3']

# 检查每一列是否存在 NaN 值
nan_columns = np.any(np.isnan(data), axis=0)

# 获取包含 NaN 值的列名
nan_column_names = [column_names[i] for i in range(len(column_names)) if nan_columns[i]]

print(f"包含 NaN 值的列名: {nan_column_names}")
```

### 完整示例
假设你有一个二维 `numpy` 数组 `data`，内容如下：

```python
import numpy as np

# 示例数据
data = np.array([
    [1, 2, np.nan],
    [4, np.nan, 6],
    [7, 8, 9]
])

# 检查每一列是否存在 NaN 值
nan_columns = np.any(np.isnan(data), axis=0)

# 获取包含 NaN 值的列索引
nan_column_indices = np.where(nan_columns)[0]

# 列名列表
column_names = ['col1', 'col2', 'col3']

# 获取包含 NaN 值的列名
nan_column_names = [column_names[i] for i in range(len(column_names)) if nan_columns[i]]

print(f"哪些列存在 NaN 值: {nan_columns}")
print(f"包含 NaN 值的列索引: {nan_column_indices}")
print(f"包含 NaN 值的列名: {nan_column_names}")
```

### 输出
```
哪些列存在 NaN 值: [False  True  True]
包含 NaN 值的列索引: [1 2]
包含 NaN 值的列名: ['col2', 'col3']
```

### 总结
- **`np.any(np.isnan(data), axis=0)`**：返回一个布尔数组，表示每一列是否存在 `NaN` 值。
- **`np.where(np.any(np.isnan(data), axis=0))[0]`**：返回包含 `NaN` 值的列索引。
- **结合列名列表**：可以获取包含 `NaN` 值的列名。

通过这些方法，你可以有效地确定 `numpy` 数组中哪些列包含 `NaN` 值，并根据需要采取进一步的处理措施。

# 全连接网络里怎么处理字符串数据？

直接将字符串数据扔进全连接网络里肯定不行！原理上根本说不通！

# 训练错误

## RuntimeError: mat1 and mat2 must have the same dtype, but got Double and Float

> Traceback (most recent call last):
>   File "D:\Learning\Experiments\ML_Study\DeepLearning\08_Dataset_and_DataLoader\Titanic_Classifier.py", line 98, in <module>
>     y_pred = model(inputs)
>   File "D:\Sofeware\anaconda\envs\ML\lib\site-packages\torch\nn\modules\module.py", line 1553, in _wrapped_call_impl
>     return self._call_impl(*args, **kwargs)
>   File "D:\Sofeware\anaconda\envs\ML\lib\site-packages\torch\nn\modules\module.py", line 1562, in _call_impl
>     return forward_call(*args, **kwargs)
>   File "D:\Learning\Experiments\ML_Study\DeepLearning\08_Dataset_and_DataLoader\Titanic_Classifier.py", line 69, in forward
>     x = self.relu(self.l1(x))
>   File "D:\Sofeware\anaconda\envs\ML\lib\site-packages\torch\nn\modules\module.py", line 1553, in _wrapped_call_impl
>     return self._call_impl(*args, **kwargs)
>   File "D:\Sofeware\anaconda\envs\ML\lib\site-packages\torch\nn\modules\module.py", line 1562, in _call_impl
>     return forward_call(*args, **kwargs)
>   File "D:\Sofeware\anaconda\envs\ML\lib\site-packages\torch\nn\modules\linear.py", line 117, in forward
>     return F.linear(input, self.weight, self.bias)
> RuntimeError: mat1 and mat2 must have the same dtype, but got Double and Float

这个错误是由于模型的输入 `inputs` 和模型权重之间的数据类型不一致造成的。你在代码中使用了 NumPy 加载数据，而 NumPy 默认使用 `float64`（Double），但 PyTorch 中的张量在进行模型前向传播时一般使用 `float32`（Float）。因此，输入数据和模型参数的数据类型不匹配，导致了错误。

### 解决方法：

你需要将输入数据的类型转换为 `float32`，这样它和模型的权重数据类型就一致了。

### 修改建议：

在 `TitanicDataset` 类中，读取数据后将 `self.x_data` 和 `self.y_data` 转换为 `float32`。

将 `TitanicDataset` 中的数据加载部分修改如下：

```python
class TitanicDataset(Dataset):
    def __init__(self, filepath):
        xy = pd.read_csv(filepath)
        xy = Pre_dataprocess1(xy)

        self.len = xy.shape[0]
        # 将 x_data 转换为 float32 类型
        self.x_data = torch.from_numpy(np.hstack((xy[:, :1], xy[:, 2:]))).float()
        # 将 y_data 转换为 float32 类型
        self.y_data = torch.from_numpy(xy[:, [1]]).float()

    def __getitem__(self, index):
        return self.x_data[index], self.y_data[index]

    def __len__(self):
        return self.len

```

## RuntimeError: mat1 and mat2 shapes cannot be multiplied (32x6 and 7x5)

训练模型的最后阶段出现了这样的错误：

> Traceback (most recent call last):
>   File "D:\Learning\Experiments\ML_Study\DeepLearning\08_Dataset_and_DataLoader\Titanic_Classifier.py", line 116, in <module>
>     outputs = model(inputs)
>   File "D:\Sofeware\anaconda\envs\ML\lib\site-packages\torch\nn\modules\module.py", line 1553, in _wrapped_call_impl
>     return self._call_impl(*args, **kwargs)
>   File "D:\Sofeware\anaconda\envs\ML\lib\site-packages\torch\nn\modules\module.py", line 1562, in _call_impl
>     return forward_call(*args, **kwargs)
>   File "D:\Learning\Experiments\ML_Study\DeepLearning\08_Dataset_and_DataLoader\Titanic_Classifier.py", line 69, in forward
>     x = self.relu(self.l1(x))
>   File "D:\Sofeware\anaconda\envs\ML\lib\site-packages\torch\nn\modules\module.py", line 1553, in _wrapped_call_impl
>     return self._call_impl(*args, **kwargs)
>   File "D:\Sofeware\anaconda\envs\ML\lib\site-packages\torch\nn\modules\module.py", line 1562, in _call_impl
>     return forward_call(*args, **kwargs)
>   File "D:\Sofeware\anaconda\envs\ML\lib\site-packages\torch\nn\modules\linear.py", line 117, in forward
>     return F.linear(input, self.weight, self.bias)
> RuntimeError: mat1 and mat2 shapes cannot be multiplied (32x6 and 7x5)

原因是测试集读取出来的特征只有6个，不符合模型的7个

## 最后处理测试集为0%

![image-20241015175312459](./assets/image-20241015175312459.png)

![image-20241015195954167](./assets/image-20241015195954167.png)

发现损失一直降不下来，维持在大概0.65左右的样子，数据集是一个很干净的数据集。

> 2024年10月15日

判断是模型泛化能力较差，需要继续学习RNN后继续完善

只用了全连接层相当于铠甲勇士打异能兽，只召唤出来头盔，没召唤出来铠甲。
