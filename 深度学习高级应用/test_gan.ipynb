{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 基于MNIST 实现对抗生成网络（GAN）",
   "id": "c99218437266086b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T13:45:37.881400Z",
     "start_time": "2025-03-19T13:45:37.862351Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np"
   ],
   "id": "eff47919a636df04",
   "outputs": [],
   "execution_count": 93
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 超参数准备",
   "id": "6b91cd0122bd4d56"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T13:45:37.931383Z",
     "start_time": "2025-03-19T13:45:37.925471Z"
    }
   },
   "cell_type": "code",
   "source": [
    "image_size = [1, 28, 28] # 样本的shape\n",
    "num_epochs = 100\n",
    "batch_size = 32\n",
    "latent_dim = 64 # 生成对抗网络里用于生成器使用的维度\n",
    "use_gpu = torch.cuda.is_available()"
   ],
   "id": "ec322dbd7e8eed18",
   "outputs": [],
   "execution_count": 94
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 生成器",
   "id": "1944a72408a0a3ee"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T13:45:37.953653Z",
     "start_time": "2025-03-19T13:45:37.931828Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 64),\n",
    "            # 引入batchnorm可以提高收敛速度，具体做法是在生成器的Linear层后面添加BatchNorm1d，最后一层除外，判别器不要加\n",
    "            torch.nn.BatchNorm1d(64),\n",
    "            # 将激活函数ReLU换成GELU效果更好\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(64, 128),\n",
    "            torch.nn.BatchNorm1d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 256),\n",
    "            torch.nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 512),\n",
    "            torch.nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, 1024),\n",
    "            torch.nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, np.prod(image_size, dtype=np.int32)),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        # z 的维度 [batch_size, latent_dim]\n",
    "        # output's shape [batch_size, 1x28x28]\n",
    "        output = self.model(z)\n",
    "        # image's shape [batch_size, 1, 28, 28]\n",
    "        # 使用*image_size可以得到类似于元组的数据[1, 28, 28] --> (1, 28, 28)\n",
    "        image = output.reshape(output.shape[0], *image_size)\n",
    "        return image"
   ],
   "id": "568ed56ed3f32184",
   "outputs": [],
   "execution_count": 95
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 判别器",
   "id": "cbaad46d19d0df5f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T13:45:37.993778Z",
     "start_time": "2025-03-19T13:45:37.981857Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        # 接受一张照片作为输入，输出一个概率值\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(np.prod(image_size, dtype=np.int32), 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid(), # 最后输出一个概率\n",
    "        )\n",
    "    def forward(self, image):\n",
    "        # shape of image: [batch_size, 1, 28, 28]\n",
    "        # 输入的shape为[batch_size, 1x28x28]\n",
    "        prob = self.model(image.reshape(image.shape[0], -1))\n",
    "        return prob"
   ],
   "id": "27381ec0f80328e7",
   "outputs": [],
   "execution_count": 96
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 数据准备",
   "id": "9a8a2a253795ceda"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T13:45:38.063336Z",
     "start_time": "2025-03-19T13:45:38.017930Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = torchvision.datasets.MNIST(\"datasets/mnist\", download=True, train=True)\n",
    "len(dataset)"
   ],
   "id": "74512f0db9569bae",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 97
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T13:45:38.120509Z",
     "start_time": "2025-03-19T13:45:38.100673Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 查看数据维度\n",
    "for i in range(len(dataset)):\n",
    "    if i < 5:\n",
    "        print(dataset[i])\n",
    "    else:\n",
    "        break"
   ],
   "id": "f71a645773b6222d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<PIL.Image.Image image mode=L size=28x28 at 0x238E2F56520>, 5)\n",
      "(<PIL.Image.Image image mode=L size=28x28 at 0x238BE7B7AF0>, 0)\n",
      "(<PIL.Image.Image image mode=L size=28x28 at 0x238BE7B7AF0>, 4)\n",
      "(<PIL.Image.Image image mode=L size=28x28 at 0x238BE7B7AF0>, 1)\n",
      "(<PIL.Image.Image image mode=L size=28x28 at 0x238BE7B7AF0>, 9)\n"
     ]
    }
   ],
   "execution_count": 98
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T13:45:38.232904Z",
     "start_time": "2025-03-19T13:45:38.190945Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 这里是一个PIL格式的数据，调用transforms改变数据的形式\n",
    "dataset = torchvision.datasets.MNIST(\"datasets/mnist\", download=True, train=True, transform=torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.Resize(28), # 转换为 28×28的\n",
    "        torchvision.transforms.ToTensor(),# 转换为Tensor\n",
    "        torchvision.transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "    ]\n",
    "))"
   ],
   "id": "24349e06d502acb",
   "outputs": [],
   "execution_count": 99
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T13:45:38.251136Z",
     "start_time": "2025-03-19T13:45:38.239188Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i in range(len(dataset)):\n",
    "    if i < 5:\n",
    "        print(dataset[i][0].shape)\n",
    "    else:\n",
    "        break"
   ],
   "id": "2b8fd49e1e022564",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "execution_count": 100
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 训练",
   "id": "bb89b429d55e4bd0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T13:45:43.875960Z",
     "start_time": "2025-03-19T13:45:38.321912Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "g_optimizer = torch.optim.Adam(generator.parameters(), lr=0.0003, betas=(0.4, 0.8), weight_decay=0.0001)\n",
    "d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=0.0003, betas=(0.4, 0.8), weight_decay=0.0001)\n",
    "\n",
    "loss_func = nn.BCELoss()\n",
    "labels_one = torch.ones(batch_size, 1)\n",
    "labels_zero = torch.zeros(batch_size, 1)\n",
    "\n",
    "if use_gpu:\n",
    "    print(\"use gpu for training\")\n",
    "    generator = generator.cuda()\n",
    "    discriminator = discriminator.cuda()\n",
    "    loss_fn = loss_func.cuda()\n",
    "    labels_one = labels_one.to(\"cuda\")\n",
    "    labels_zero = labels_zero.to(\"cuda\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, mini_batch in enumerate(dataloader): # i是索引，mini_batch 是每一个样本，包含数据和标签\n",
    "        true_images, _ = mini_batch\n",
    "        z = torch.randn(batch_size, latent_dim) # 符合高斯分布的随机分布\n",
    "        if use_gpu:\n",
    "            true_images = true_images.to(\"cuda\")\n",
    "            z = z.to(\"cuda\")\n",
    "        pred_images = generator(z)\n",
    "\n",
    "        '''\n",
    "            对于D是要最大化，最大化的内容如下：\n",
    "            1. 判断真实数据为真实数据的概率\n",
    "            2. 判断来自生成器生成的数据的为虚假数据的概率\n",
    "\n",
    "            对于G是要最小化，最小化的内容如下：\n",
    "            1. 判别器判断来自生成器的数据为真实数据的概率最大\n",
    "        '''\n",
    "        # ------------------------------------------------------------------------\n",
    "        # 生成器优化\n",
    "        g_optimizer.zero_grad()\n",
    "\n",
    "        recons_loss = torch.abs(pred_images-true_images).mean()\n",
    "\n",
    "        # 对于G是要最小化，最小化的内容如下：\n",
    "        # 1. 判别器判断来自生成器的数据为真实数据的概率最大\n",
    "        g_loss = recons_loss*0.05 + loss_func(discriminator(pred_images), labels_one) # 这里discriminator(pred_images)是输出生成器是真实数据的概率。所以这里就是输出概率与1的差异\n",
    "\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "\n",
    "        # ------------------------------------------------------------------------\n",
    "        # 判别器优化\n",
    "        d_optimizer.zero_grad()\n",
    "\n",
    "        # 对于D是要最大化，最大化的内容如下：\n",
    "        #   1. 判断真实数据为真实数据的概率  loss_func(discriminator(true_images), torch.ones(batch_size, 1))\n",
    "        #   2. 判断来自生成器生成的数据的为虚假数据的概率  loss_func(discriminator(pred_images.detach()), torch.ones(batch_size, 0))\n",
    "        # pred_images.detach() 因为是在更新判别器的参数，就不需要更新生成器的参数，因此需要将生成器的内容从计算图中剥离出来\n",
    "        # real_loss基于真实图片，fake_loss基于生成图片\n",
    "        real_loss = loss_fn(discriminator(true_images), labels_one)\n",
    "        fake_loss = loss_fn(discriminator(pred_images.detach()), labels_zero)\n",
    "        d_loss = (real_loss + fake_loss)\n",
    "        # d_loss = d_loss / 2 # 取一个平均\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "\n",
    "        # # 每隔1000步打印一次结果\n",
    "        # if i % 1000 == 0:\n",
    "        #     for index, image in enumerate(pred_images):\n",
    "        #         torchvision.utils.save_image(image, f\"image_{index}.png\")\n",
    "        if i % 50 == 0:\n",
    "            print(f\"step:{len(dataloader)*epoch+i}, recons_loss:{recons_loss.item()}, g_loss:{g_loss.item()}, d_loss:{d_loss.item()}, real_loss:{real_loss.item()}, fake_loss:{fake_loss.item()}\")\n",
    "\n",
    "        if i % 400 == 0:\n",
    "            image = pred_images[:16].data\n",
    "            torchvision.utils.save_image(image, f\"gan_mnist_output/image_{len(dataloader)*epoch+i}.png\", nrow=4)\n",
    "\n",
    "\n"
   ],
   "id": "76eb946c654bb4fd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use gpu for training\n",
      "step:0, recons_loss:0.9491025805473328, g_loss:0.7523645162582397, d_loss:1.3791230916976929, real_loss:0.6976013779640198, fake_loss:0.6815217137336731\n",
      "step:50, recons_loss:0.5167753100395203, g_loss:0.777081310749054, d_loss:1.0720994472503662, real_loss:0.4336715638637543, fake_loss:0.6384278535842896\n",
      "step:100, recons_loss:0.47661033272743225, g_loss:1.0258558988571167, d_loss:1.1103706359863281, real_loss:0.6527243852615356, fake_loss:0.4576461911201477\n",
      "step:150, recons_loss:0.6542694568634033, g_loss:2.138892889022827, d_loss:0.38093554973602295, real_loss:0.250055193901062, fake_loss:0.13088034093379974\n",
      "step:200, recons_loss:0.5037420988082886, g_loss:0.2634848654270172, d_loss:1.608109474182129, real_loss:0.051162030547857285, fake_loss:1.5569474697113037\n",
      "step:250, recons_loss:0.6073771119117737, g_loss:2.094127655029297, d_loss:0.2906925678253174, real_loss:0.1546083241701126, fake_loss:0.13608425855636597\n",
      "step:300, recons_loss:0.6044324040412903, g_loss:3.4533040523529053, d_loss:0.29640552401542664, real_loss:0.26280900835990906, fake_loss:0.03359650820493698\n",
      "step:350, recons_loss:0.5391454696655273, g_loss:0.10565483570098877, d_loss:2.644888401031494, real_loss:0.010671526193618774, fake_loss:2.634216785430908\n",
      "step:400, recons_loss:0.534281849861145, g_loss:1.6697322130203247, d_loss:0.30954819917678833, real_loss:0.09418956935405731, fake_loss:0.2153586447238922\n",
      "step:450, recons_loss:0.5369486212730408, g_loss:2.932312250137329, d_loss:0.4277552664279938, real_loss:0.3713824152946472, fake_loss:0.056372858583927155\n",
      "step:500, recons_loss:0.5153214335441589, g_loss:1.4363914728164673, d_loss:0.37688928842544556, real_loss:0.09293963015079498, fake_loss:0.2839496433734894\n",
      "step:550, recons_loss:0.543290913105011, g_loss:1.3274089097976685, d_loss:0.3872988224029541, real_loss:0.06710147857666016, fake_loss:0.32019734382629395\n",
      "step:600, recons_loss:0.44499751925468445, g_loss:0.3323898911476135, d_loss:1.365078091621399, real_loss:0.03536781296133995, fake_loss:1.3297102451324463\n",
      "step:650, recons_loss:0.46801140904426575, g_loss:2.3482720851898193, d_loss:0.32093384861946106, real_loss:0.21799080073833466, fake_loss:0.1029430478811264\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[101], line 20\u001B[0m\n\u001B[0;32m     17\u001B[0m     labels_zero \u001B[38;5;241m=\u001B[39m labels_zero\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_epochs):\n\u001B[1;32m---> 20\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i, mini_batch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(dataloader): \u001B[38;5;66;03m# i是索引，mini_batch 是每一个样本，包含数据和标签\u001B[39;00m\n\u001B[0;32m     21\u001B[0m         true_images, _ \u001B[38;5;241m=\u001B[39m mini_batch\n\u001B[0;32m     22\u001B[0m         z \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mrandn(batch_size, latent_dim) \u001B[38;5;66;03m# 符合高斯分布的随机分布\u001B[39;00m\n",
      "File \u001B[1;32mD:\\Sofeware\\anaconda\\envs\\ML\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    627\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    628\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    629\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 630\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    631\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    632\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    633\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    634\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[1;32mD:\\Sofeware\\anaconda\\envs\\ML\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:673\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    671\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    672\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 673\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    674\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[0;32m    675\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[1;32mD:\\Sofeware\\anaconda\\envs\\ML\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     50\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[0;32m     51\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 52\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[0;32m     53\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     54\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[1;32mD:\\Sofeware\\anaconda\\envs\\ML\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     50\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[0;32m     51\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 52\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[0;32m     53\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     54\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[1;32mD:\\Sofeware\\anaconda\\envs\\ML\\lib\\site-packages\\torchvision\\datasets\\mnist.py:146\u001B[0m, in \u001B[0;36mMNIST.__getitem__\u001B[1;34m(self, index)\u001B[0m\n\u001B[0;32m    143\u001B[0m img \u001B[38;5;241m=\u001B[39m Image\u001B[38;5;241m.\u001B[39mfromarray(img\u001B[38;5;241m.\u001B[39mnumpy(), mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mL\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    145\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 146\u001B[0m     img \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    148\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_transform \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    149\u001B[0m     target \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_transform(target)\n",
      "File \u001B[1;32mD:\\Sofeware\\anaconda\\envs\\ML\\lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001B[0m, in \u001B[0;36mCompose.__call__\u001B[1;34m(self, img)\u001B[0m\n\u001B[0;32m     93\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, img):\n\u001B[0;32m     94\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransforms:\n\u001B[1;32m---> 95\u001B[0m         img \u001B[38;5;241m=\u001B[39m \u001B[43mt\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     96\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m img\n",
      "File \u001B[1;32mD:\\Sofeware\\anaconda\\envs\\ML\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Sofeware\\anaconda\\envs\\ML\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32mD:\\Sofeware\\anaconda\\envs\\ML\\lib\\site-packages\\torchvision\\transforms\\transforms.py:277\u001B[0m, in \u001B[0;36mNormalize.forward\u001B[1;34m(self, tensor)\u001B[0m\n\u001B[0;32m    269\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, tensor: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m    270\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    271\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[0;32m    272\u001B[0m \u001B[38;5;124;03m        tensor (Tensor): Tensor image to be normalized.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    275\u001B[0m \u001B[38;5;124;03m        Tensor: Normalized Tensor image.\u001B[39;00m\n\u001B[0;32m    276\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 277\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnormalize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtensor\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmean\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstd\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minplace\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Sofeware\\anaconda\\envs\\ML\\lib\\site-packages\\torchvision\\transforms\\functional.py:350\u001B[0m, in \u001B[0;36mnormalize\u001B[1;34m(tensor, mean, std, inplace)\u001B[0m\n\u001B[0;32m    347\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(tensor, torch\u001B[38;5;241m.\u001B[39mTensor):\n\u001B[0;32m    348\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mimg should be Tensor Image. Got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(tensor)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 350\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF_t\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnormalize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtensor\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmean\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmean\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstd\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstd\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minplace\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minplace\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Sofeware\\anaconda\\envs\\ML\\lib\\site-packages\\torchvision\\transforms\\_functional_tensor.py:920\u001B[0m, in \u001B[0;36mnormalize\u001B[1;34m(tensor, mean, std, inplace)\u001B[0m\n\u001B[0;32m    918\u001B[0m mean \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mas_tensor(mean, dtype\u001B[38;5;241m=\u001B[39mdtype, device\u001B[38;5;241m=\u001B[39mtensor\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[0;32m    919\u001B[0m std \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mas_tensor(std, dtype\u001B[38;5;241m=\u001B[39mdtype, device\u001B[38;5;241m=\u001B[39mtensor\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m--> 920\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\u001B[43mstd\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m)\u001B[38;5;241m.\u001B[39many():\n\u001B[0;32m    921\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstd evaluated to zero after conversion to \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdtype\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, leading to division by zero.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    922\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m mean\u001B[38;5;241m.\u001B[39mndim \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 101
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 注意点",
   "id": "f512f432bdf1c7c0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1. 引入batchnorm可以提高收敛速度，具体做法是在生成器的Linear层后面添加BatchNorm1d，最后一层除外，判别器不要加\n",
    "2. 直接预测【0,1】之间的像素值即可，不做归一化的transform；或者也可以放大，预测【-1,1】之间，用mean=0.5 std=0.5进行归一化transform都可以\n",
    "3. 将激活函数ReLU换成GELU效果更好\n",
    "4. real_loss基于真实图片，fake_loss基于生成图片，real_loss = loss_fn(discriminator(gt_images), torch.ones(batch_size, 1))，fake_loss = loss_fn(discriminator(pred_images.detach()), torch.zeros(batch_size, 1))\n",
    "5. 适当引入重构loss，计算像素值的L1误差\n",
    "6. 建议引入loss打印语句，如：print(f\"step:{len(dataloader)*epoch+i}, g_loss:{g_loss.item()}, d_loss:{d_loss.item()}, real_loss:{real_loss.item()}, fake_loss:{fake_loss.item()}\")\n",
    "7. 判别器模型容量不宜过大\n",
    "8. save_image中的normalize设置成True，目的是将像素值min-max自动归一到【0,1】范围内，如果已经预测了【0,1】之间，则可以不用设置True\n",
    "9. 判别器的学习率不能太小\n",
    "10. Adam的一阶平滑系数和二阶平滑系数 betas 适当调小一点，可以帮助学习，设置一定比例的weight decay"
   ],
   "id": "e62c979081d0462a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
